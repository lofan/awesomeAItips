The Generative AI Guide
Based on the infographic provided, here are the items categorized into the four requested sections:

1. GenAI Technologies
* Transformers: Neural network architecture powering most modern LLMs and generative models.
* Diffusion Model: Generative models that create data by reversing noise processes.

Large Language Models (LLMs): AI models trained on massive text datasets to generate human-like text.

Multi-Modal Models: Models that process and generate multiple data types like text, image, and audio.

Self-Supervised Learning: Training models on unlabeled data by predicting parts of the input.

Reinforcement Learning from Human Feedback (RLHF): Aligns AI behavior with human preference through feedback loops.

Artificial Intelligence: Broad field of creating machines that can perform human-like cognitive tasks.

Machine Learning: Training algorithms to learn patterns and make predictions from data.

Prompt Engineering: Crafting inputs to optimize AI model responses.

GANs (Generative Adversarial Networks): Two-network systems competing to produce realistic data.

2. Using the Model APIs
API Best Practices: Guidelines for efficient, secure, and scalable API use.

OpenAI API: Access to GPT models for chat, code, and image generation.

Hugging Face: Open platform for models, datasets, and NLP tools.

Vertex AI: Google's managed ML platform for training and deploying models.

LangChain: Framework for building LLM-powered apps with context and tool use.

Replicate: Cloud service to run and deploy AI models via API.

Cohere: API for NLP tasks like classification, summarization, and embedding generation.

Anthropic Claude API: Safe and controllable AI model for text-based tasks.

Mistral API: High performance, open-weight LLMs optimized for efficiency.

Azure OpenAI: API for large language models and advanced text manipulation.

3. Making Models Your Own
Knowledge Injection: Embedding domain-specific knowledge directly into model insights.

Synthetic Data Generation: Create custom datasets to improve training.

Parameter-Efficient Tuning: Update fewer parameters for cost-effective customization.

Custom Tokenizers: Adjust tokenization rules to better handle domain-specific text.

Domain-Specific Training: Specializing a model on niche industry data.

Adapter Layers: Small trainable modules inserted into frozen model layers.

Fine-Tuning: Retrain parts of a model on specific data for customization.

RAG (Retrieval-Augmented Generation): Enhance responses by integrating real-time retrieved knowledge.

LoRA (Low-Rank Adaptation): Efficient fine-tuning by updating small rank matrices.

Prefix Tuning: Add trainable prefixes to prompts without retraining the entire model.

4. Advanced GenAI Techniques
Model Distillation: Compress large models into smaller, faster ones without major loss in accuracy.

Curriculum Learning: Train models with progressively more complex tasks for better learning.

AutoML: Automating model selection, training, and deployment.

Self-Improving Loops: AI improves its output iteratively using feedback.

Vector Databases (Pinecone, Weaviate): Store and search embeddings for semantic similarity.

RAG (Retrieval-Augmented Generation): Combine LLMs with external knowledge retrieval for accurate answers.

Fine-Tuning Strategies: Adapt large models to specific domains or tasks.

Prompt Chaining: Linking multiple prompts to perform multi-step reasoning.

Tool Use & Function Calling: Enable models to interact with external tools and APIs.

Multi-Agent Orchestration: Coordinate multiple AI agents for complex workflows.
