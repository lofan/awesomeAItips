## Some AI Papers ##
The field of AI moves incredibly fast, but there are several foundational "pillars" that every researcher or enthusiast should read to understand how we reached the current era of Large Language Models (LLMs) and Generative AI.

I have categorized these into the major shifts in the industry:

---

## 1. The "Architecture" Shift (Transformers & LLMs)

These papers moved AI away from processing data step-by-step (Recurrent Neural Networks) to processing everything in parallel using "Attention."

* **Attention Is All You Need (2017)**
* **Why read it:** This is arguably the most important paper in modern AI. It introduced the **Transformer** architecture, which is the backbone of ChatGPT, Claude, and Gemini.
* **Link:** [Read on arXiv](https://arxiv.org/abs/1706.03762)


* **BERT: Pre-training of Deep Bidirectional Transformers (2018)**
* **Why read it:** It showed how to train a model to understand the *context* of words by looking at them from both directions (left-to-right and right-to-left).
* **Link:** [Read on arXiv](https://arxiv.org/abs/1810.04805)


* **Language Models are Few-Shot Learners (GPT-3 Paper, 2020)**
* **Why read it:** This paper proved that if you make a model big enough, it can perform tasks it wasn't specifically trained for (like coding or translating) just by being given a few examples.
* **Link:** [Read on arXiv](https://arxiv.org/abs/2005.14165)



---

## 2. The "Computer Vision" Foundations

Before LLMs, the biggest breakthroughs were in how machines "see" and recognize images.

* **ImageNet Classification with Deep Convolutional Neural Networks (AlexNet, 2012)**
* **Why read it:** This is the "Big Bang" of modern Deep Learning. It proved that GPUs and deep neural networks could outperform every other method in image recognition.
* **Link:** [Read on NIPS](https://proceedings.neurips.cc/paper/2012/file/c399862d3b9d6b76c8436e924a68c45b-Paper.pdf)


* **Deep Residual Learning for Image Recognition (ResNet, 2015)**
* **Why read it:** It solved the problem of "vanishing gradients," allowing us to train networks that were hundreds of layers deep (previous ones broke after about 20).
* **Link:** [Read on arXiv](https://arxiv.org/abs/1512.03385)



---

## 3. Generative AI & Creativity

These papers laid the groundwork for AI that can *create* new images and text rather than just classifying them.

* **Generative Adversarial Nets (GANs, 2014)**
* **Why read it:** Introduced the idea of two AIs "fighting" each otherâ€”one tries to create a fake image, and the other tries to catch it. This led to the first high-quality AI-generated faces.
* **Link:** [Read on arXiv](https://arxiv.org/abs/1406.2661)


* **High-Resolution Image Synthesis with Latent Diffusion Models (Stable Diffusion, 2021)**
* **Why read it:** This is the math behind modern image generators. It explains how to turn random "noise" into a crisp, high-definition image based on a text prompt.
* **Link:** [Read on arXiv](https://arxiv.org/abs/2112.10752)



---

## 4. Modern Reasoning & Efficiency

As models have grown, the focus has shifted to making them smarter (Reasoning) and smaller (Efficient).

* **Chain-of-Thought Prompting Elicits Reasoning (2022)**
* **Why read it:** It discovered that asking an AI to "think step-by-step" drastically improves its ability to solve math and logic problems.
* **Link:** [Read on arXiv](https://arxiv.org/abs/2201.11903)


* **LoRA: Low-Rank Adaptation of Large Language Models (2021)**
* **Why read it:** This paper is the reason why people can run "fine-tuned" versions of Llama on their home computers. It made updating huge models much cheaper and faster.
* **Link:** [Read on arXiv](https://arxiv.org/abs/2106.09685)



---

**Would you like me to provide a summary of a specific paper from this list, or perhaps a roadmap on the best order to read them?**
